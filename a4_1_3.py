# -*- coding: utf-8 -*-
"""A4_1_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P01XV6hEHTYepcTf9CY5oCroKkcnKcG6
"""

pip install transformers

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

model.generation_config

pip install treelib

from treelib import Node, Tree

tree = Tree()

prompt = "It is important for all countries to try harder to reduce carbon emissions because"
original_input_ids = tokenizer(prompt, return_tensors="pt").input_ids

def create_Node(inputids, depth, p, t, tree, branchlist):
  if depth == 0:
    return
  else:
    torch.manual_seed(0)
    print(inputids.shape[1])
    print(tokenizer.batch_decode(inputids, skip_special_tokens=True))
    ModelOutput = model.generate(inputids, do_sample=True, max_length = inputids.shape[1] + 1, temperature = t, top_p = p, return_dict_in_generate = True, output_scores=True)
    score = ModelOutput.scores[0]
    sorted_probs, sorted_indices = torch.sort(torch.softmax(score, dim = 1)) #convert to probability, sort
    sorted_probs = torch.flip(sorted_probs, [1])
    sorted_indices = torch.flip(sorted_indices, [1]) #flip indices to match
    cumulative_probs = torch.cumsum(sorted_probs, 1) #put into descending order, take cumulative sum
    greater_than_p = False
    indices_to_keep = []
    probs_to_keep = []

    for i in range(3):
      if not greater_than_p:
        indices_to_keep += [int(sorted_indices[0][i])]
        probs_to_keep += [round(1000000*float(sorted_probs[0][i]))/1000000]
      if cumulative_probs[0][i] >= p:
        greater_than_p = True
    words_to_keep = tokenizer.batch_decode(torch.tensor([indices_to_keep]), skip_special_tokens=True)[0].strip().split(' ')
    for i in range(min(len(words_to_keep), 3)):
      name = words_to_keep[i] + " " +str(probs_to_keep[i])
      id = str(branchlist + [i])
      parent = str(branchlist) #branchlist the list of which branches were taken, from the top down.
      tree.create_node(name, id, parent)
      new_input_ids = torch.cat((inputids.squeeze(), torch.tensor([indices_to_keep[i]])),0).unsqueeze(0)
      create_Node(new_input_ids, depth-1, p, t, tree, branchlist + [i])
  return

tree = Tree()
tree.create_node("Prompt", "[]")
create_Node(original_input_ids, 6, 1.0, 0.1, tree, [])
tree.save2file("t = 0.1, p = 0.1 depth 6.txt")


tree = Tree()
tree.create_node("Prompt", "[]")
create_Node(original_input_ids, 4, 1.0, 1.5, tree, [])
tree.save2file("t = 1.5, p = 1.0 depth 4.txt")