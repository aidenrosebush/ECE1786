# -*- coding: utf-8 -*-
"""A1P3_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gBxIl935UysFi4_EZGBwILj46WfG8vk5
"""

class Word2vecModel(torch.nn.Module):
    def __init__(self, vocab_size, embedding_size):
        super().__init__()
        self.embedding = torch.rand((vocab_size, embedding_size), requires_grad = True)

    def forward(self, x):
        #given the word (index of the word) x we take the inner product with every other column of the embedding tensor
        if len(self.embedding[x].shape) == 2: #batches (training)
          logits = torch.einsum('kj, ij -> ki', self.embedding[x], self.embedding) #inner products
        elif len(self.embedding[x].shape) == 1: #single samples
          logits = torch.einsum('j, ij -> i', self.embedding[x], self.embedding)
        return logits