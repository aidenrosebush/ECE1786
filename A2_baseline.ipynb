{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkDmg2I7odhK"
      },
      "outputs": [],
      "source": [
        "from numpy.core.fromnumeric import mean\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "### 3.3 Processing of the data ###\n",
        "# 3.3.1\n",
        "# The first time you run this will download a 862MB size file to .vector_cache/glove.6B.zip\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100) # embedding size = 100\n",
        "\n",
        "# TextDataset is Described in Section 3.3 of Assignment 2\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, split=\"train\"):\n",
        "        data_path = \"data\"\n",
        "        df = pd.read_csv(os.path.join(data_path, f\"{split}.tsv\"), sep=\"\\t\")\n",
        "\n",
        "        # X: torch.tensor (maxlen, batch_size), padded indices\n",
        "        # Y: torch.tensor of len N\n",
        "        X, Y = [], []\n",
        "        V = len(vocab.vectors)\n",
        "        for i, row in df.iterrows():\n",
        "            L = row.values[0].split()\n",
        "            X.append(torch.tensor([vocab.stoi.get(w, V-1) for w in L]))  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
        "            Y.append(float(row.values[1]))\n",
        "        self.X = X\n",
        "        self.Y = torch.tensor(Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "# my_collate_function prepares batches\n",
        "# it also pads each batch with zeroes.\n",
        "\n",
        "class baseline(torch.nn.Module):\n",
        "    def __init__(self, vocab, device):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding.from_pretrained(vocab.vectors).to(device)\n",
        "      self.linear = nn.Linear(100,1).to(device)\n",
        "    def forward(self, x):\n",
        "      return self.linear(torch.mean(self.embedding(x), dim = 1)).squeeze(1)\n",
        "\n",
        "def my_collate_function(batch, device):\n",
        "    # Handle the padding here\n",
        "    # batch is approximately: [dataset[i] for i in range(0, batch_size)]\n",
        "    # Since the dataset[i]'s contents is defined in the __getitem__() above, this collate function\n",
        "    # should be set correspondingly.\n",
        "    # Also: collate_function just takes one argument. To pass in additional arguments (e.g., device),\n",
        "    # we need to wrap up an anonymous function (using lambda below)\n",
        "    batch_x, batch_y = [], []\n",
        "    max_len = 0\n",
        "    for x,y in batch:\n",
        "        batch_y += [y]\n",
        "        max_len = max(max_len, len(x))\n",
        "    for x,y in batch:\n",
        "        x_p = torch.concat(\n",
        "            [x, torch.zeros(max_len - len(x))]\n",
        "        )\n",
        "        batch_x.append(x_p)\n",
        "    return torch.stack(batch_x).int().to(device), torch.tensor(batch_y).to(device)\n",
        "\n",
        "\n",
        "def train_baseline(epochs, learning_rate, train_dataloader, val_dataloader, vocab, device):\n",
        "  net = baseline(vocab, device)\n",
        "  opt = optim.Adam(net.parameters(), lr = learning_rate)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "  sigmoid = nn.Sigmoid()\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  train_errors = []\n",
        "  val_errors = []\n",
        "  epochslist = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \"+str(epoch))\n",
        "    trainloss = []\n",
        "    valloss = []\n",
        "    trainerr = 0\n",
        "    valerr = 0\n",
        "    iter = 0 #count how many batches\n",
        "    for sentences, labels in train_dataloader:\n",
        "      opt.zero_grad()\n",
        "      logits = net(sentences.to(device))\n",
        "      loss = criterion(logits, labels.to(device))\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      trainloss += [float(loss.item())]\n",
        "      corr = torch.round(sigmoid(logits)) != labels\n",
        "      trainerr += int(corr.sum())\n",
        "      iter += len(corr)\n",
        "    train_errors += [trainerr/iter]\n",
        "    train_losses += [mean(trainloss)]\n",
        "    iter = 0\n",
        "    for sentences, labels in val_dataloader:\n",
        "      logits = net(sentences)\n",
        "      loss = criterion(logits, labels.to(device))\n",
        "      valloss += [float(loss.item())]\n",
        "      corr = torch.round(sigmoid(logits)) != labels\n",
        "      valerr += int(corr.sum())\n",
        "      iter += len(corr)\n",
        "    val_errors += [valerr/iter]\n",
        "    val_losses += [mean(valloss)]\n",
        "    epochslist += [epoch]\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(epochslist, train_losses, label = 'Training Loss')\n",
        "  ax.plot(epochslist, val_losses, label = 'Validation Loss')\n",
        "  ax.set_title(\"Training and Validation Loss\")\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "  ax.set_ylabel(\"Loss\")\n",
        "  ax.legend()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(epochslist, train_errors, label = 'Training Errors')\n",
        "  ax.plot(epochslist, val_errors, label = 'Validation Errors')\n",
        "  ax.set_title(\"Training and Validation Errors\")\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "  ax.set_ylabel(\"Error\")\n",
        "  ax.legend()\n",
        "\n",
        "  return net\n",
        "def evaluate(net, dataloader):\n",
        "  sigmoid = nn.Sigmoid()\n",
        "\n",
        "  iter = 0\n",
        "  err = 0\n",
        "  for sentences, labels in dataloader:\n",
        "    iter += 1\n",
        "    logits = net(sentences)\n",
        "    corr = torch.round(sigmoid(logits)) != labels\n",
        "    err += int(corr.sum())\n",
        "  accuracy = err/(iter*len(labels))\n",
        "  print(\"Final accuracy: \"+str(accuracy))\n",
        "  return accuracy\n",
        "\n",
        "#initialize dataloaders and device\n",
        "batch_size = 50\n",
        "#fix seed\n",
        "torch.manual_seed(2)\n",
        "#set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print (\"Using device:\", device)\n",
        "\n",
        "# 3.3.2\n",
        "train_dataset = TextDataset(glove, \"train\")\n",
        "val_dataset = TextDataset(glove, \"val\")\n",
        "test_dataset = TextDataset(glove, \"test\")\n",
        "overfit_dataset = TextDataset(glove, \"overfit\")\n",
        "\n",
        "# 3.3.3\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "overfit_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=overfit_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "#Run these functions to test my code\n",
        "def train_overfit(): #4.4\n",
        "  net = train_baseline(50, 0.001, overfit_dataloader, val_dataloader, glove, device)\n",
        "  return net\n",
        "def train_full(): #4.5 and 4.7\n",
        "  net = train_baseline(50, 0.001, train_dataloader, val_dataloader, glove, device)\n",
        "  evaluate(net, test_dataloader)\n",
        "  torch.save(net.state_dict(), 'model baseline.pt')\n",
        "  return net\n",
        "def get_closest_words(): #4.6\n",
        "  for name, param in (net.named_parameters()): #get weights\n",
        "    if name == \"linear.weight\":\n",
        "      weights = param.data\n",
        "\n",
        "  dists = torch.cosine_similarity(glove.vectors.to(device), weights.to(device))    # compute distances to all words\n",
        "  lst = sorted(enumerate(dists), key=lambda x: x[1]) # sort by distance\n",
        "  for i in range(len(lst) -2, len(lst)- (20+2), -1):    # take the top n, don't consider top result (will be target itself)\n",
        "    idx = lst[i][0]\n",
        "    difference = lst[i][1]\n",
        "    print(glove.itos[idx], \"\\t%5.2f\" % lst[i][1])\n",
        "  return"
      ]
    }
  ]
}