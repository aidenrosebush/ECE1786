# -*- coding: utf-8 -*-
"""A1P1_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16VUN9V83UeSwzHQCVpMme_S75UsNKUOx
"""

#import torch
#import torchtext

# Still must import glove- commented out imports since I expect this to be run somewhere automatically with necessary imports made
#glove = torchtext.vocab.GloVe(name="6B", dim=50), # trained on Wikipedia 2014 corpus. embedding size = 50

def print_closest_cosine_words(vec, n=5):
    dists = torch.cosine_similarity(glove.vectors, vec.unsqueeze(0))    # compute distances to all words
    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance
    for i in range(len(lst) -2, len(lst)- (n+2), -1):    # take the top n, don't consider top result (will be target itself)
        idx = lst[i][0]
        difference = lst[i][1]
        print(glove.itos[idx], "\t%5.2f" % lst[i][1])

def print_closest_words(vec, n=5):
    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words
    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance
    for idx, difference in lst[1:n+1]:   # take the top n
        print(glove.itos[idx] + "\t%5.2f" % difference)

countries = ['canada','mexico','USA','china','england','greece','spain','portugal','norway','finland']

print("with euclidean similarity: ")
for c in countries:
  print_closest_words(glove['paris']-glove['france'] + glove[c], n=1)

print("with cosine similarity: ")
for c in countries:
  print_closest_cosine_words(glove['paris']-glove['france'] + glove[c], n=1)