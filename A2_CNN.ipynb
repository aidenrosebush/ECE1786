{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0QZno_pHZVY",
        "outputId": "58ad3554-0be7-4240-bc2a-53283593be20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#to use this notebook, run all cells containing a def statement and then call the various functions in the very last cell.\n",
        "#sample usages of this notebook are included there.\n",
        "\n",
        "from numpy.core.fromnumeric import mean\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100)\n",
        "\n",
        "#copied data handling and initialization functions from section 3 and 4\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab, split=\"train\"):\n",
        "        data_path = \"data\"\n",
        "        df = pd.read_csv(os.path.join(data_path, f\"{split}.tsv\"), sep=\"\\t\")\n",
        "        X, Y = [], []\n",
        "        V = len(vocab.vectors)\n",
        "        for i, row in df.iterrows():\n",
        "            L = row.values[0].split()\n",
        "            X.append(torch.tensor([vocab.stoi.get(w, V-1) for w in L]))  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
        "            Y.append(float(row.values[1]))\n",
        "        self.X = X\n",
        "        self.Y = torch.tensor(Y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "def my_collate_function(batch, device):\n",
        "    batch_x, batch_y = [], []\n",
        "    max_len = 0\n",
        "    for x,y in batch:\n",
        "        batch_y += [y]\n",
        "        max_len = max(max_len, len(x))\n",
        "    for x,y in batch:\n",
        "        x_p = torch.concat(\n",
        "            [x, torch.zeros(max_len - len(x))]\n",
        "        )\n",
        "        batch_x.append(x_p)\n",
        "    return torch.stack(batch_x).int().to(device), torch.tensor(batch_y).to(device)\n",
        "\n",
        "#initialize dataloaders and device\n",
        "batch_size = 10\n",
        "#fix seed\n",
        "torch.manual_seed(2)\n",
        "#set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 3.3.2\n",
        "train_dataset = TextDataset(glove, \"train\")\n",
        "val_dataset = TextDataset(glove, \"val\")\n",
        "test_dataset = TextDataset(glove, \"test\")\n",
        "overfit_dataset = TextDataset(glove, \"overfit\")\n",
        "\n",
        "# 3.3.3\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "overfit_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset=overfit_dataset,\n",
        "    batch_size= batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: my_collate_function(batch, device))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new CNN class\n",
        "class CNN_classifier(torch.nn.Module):\n",
        "  def __init__(self, vocab, device, k1, k2, n1, n2):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding.from_pretrained(vocab.vectors, freeze = False).to(device)\n",
        "    self.conv1 = nn.Conv2d(1,n1,(k1, 100),1, bias = False).to(device)\n",
        "    self.conv2 = nn.Conv2d(1,n2,(k2, 100),1, bias = False).to(device)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.maxpool = nn.MaxPool1d(2) #sentence length varies between batches so just use a kernel size of two repeatedly\n",
        "    self.linear = nn.Linear(n1+n2,1).to(device)\n",
        "  def forward(self, x):\n",
        "    s = self.embedding(x).unsqueeze(1) #expect input in the form (N,C,H,W); only one channel here\n",
        "    c1 = self.relu(self.conv1(s).squeeze(3))\n",
        "    while c1.shape[2] > 1:\n",
        "      c1 = self.maxpool(c1)\n",
        "    c1 = c1.squeeze(1)\n",
        "\n",
        "    c2 = self.relu(self.conv1(s).squeeze(3))\n",
        "    while c2.shape[2] > 1:\n",
        "      c2 = self.maxpool(c2)\n",
        "    c2 = c2.squeeze(1)\n",
        "    maxes = torch.cat((c1,c2), 1).squeeze()\n",
        "    return self.linear(maxes).squeeze()"
      ],
      "metadata": {
        "id": "fkPWeGaCXAWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(net, test_dataloader, criterion, sigmoid):\n",
        "  iter = 0\n",
        "  err = 0\n",
        "  ll = []\n",
        "  for sentences, labels in test_dataloader:\n",
        "    logits = net(sentences)\n",
        "    loss = criterion(logits, labels)\n",
        "    corr = torch.round(sigmoid(logits)) != labels\n",
        "    iter += len(corr)\n",
        "    err += int(corr.sum())\n",
        "    ll += [float(loss.item())]\n",
        "  return mean(ll), err/iter"
      ],
      "metadata": {
        "id": "Ur1LsD4d9yPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(k1, k2, n1, n2, epochs, lr, train_dataloader, val_dataloader, test_dataloader): #5.1 and 5.2\n",
        "  net = CNN_classifier(glove, device, k1, k2, n1, n2)\n",
        "  opt = optim.Adam(net.parameters(), lr = lr)\n",
        "  criterion = torch.nn.BCEWithLogitsLoss()\n",
        "  sigmoid = nn.Sigmoid()\n",
        "  iter = 0\n",
        "\n",
        "  tl = [] #training losses (epochs)\n",
        "  vl = [] #validation losses (epochs)\n",
        "  te = [] #training error (epochs)\n",
        "  ve = [] #validation error (epochs)\n",
        "  epochslist = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    tll = [] #training loss each epoch\n",
        "    vll = [] #validation loss each epoch\n",
        "    trainerr = 0.0\n",
        "    valerr = 0.0\n",
        "    iter = 0\n",
        "\n",
        "    for sentences, labels in train_dataloader:\n",
        "      opt.zero_grad()\n",
        "      logits = net(sentences)\n",
        "      loss = criterion(logits, labels)\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "      tll += [float(loss.item())]\n",
        "      corr = torch.round(sigmoid(logits)) != labels\n",
        "      iter += len(corr)\n",
        "      trainerr += int(corr.sum())\n",
        "    tl += [mean(tll)]\n",
        "    te += [trainerr/iter]\n",
        "\n",
        "    vll, err = eval(net, val_dataloader, criterion, sigmoid)\n",
        "    vl += [vll]\n",
        "    ve += [err]\n",
        "    epochslist += [epoch]\n",
        "\n",
        "  testloss, testerr = eval(net, test_dataloader, criterion, sigmoid)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(epochslist, tl, label = 'Training Loss')\n",
        "  ax.plot(epochslist, vl, label = 'Validation Loss')\n",
        "  ax.set_title(\"Training and Validation Loss\")\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "  ax.set_ylabel(\"Loss\")\n",
        "  ax.legend()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(epochslist, te, label = 'Training Errors')\n",
        "  ax.plot(epochslist, ve, label = 'Validation Errors')\n",
        "  ax.set_title(\"Training and Validation Errors\")\n",
        "  ax.set_xlabel(\"Epoch\")\n",
        "  ax.set_ylabel(\"Error\")\n",
        "  ax.legend()\n",
        "\n",
        "  print(\"Testing loss: \"+str(testloss))\n",
        "  print(\"Testing accuracy: \"+str(1-testerr))\n",
        "\n",
        "  return net"
      ],
      "metadata": {
        "id": "o7-mGHaB9MY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_words(net): #5.3\n",
        "  for name, param in (net.named_parameters()): #get weights\n",
        "    if name == \"conv1.weight\":\n",
        "      weights1 = param.data\n",
        "    elif name == \"conv2.weight\":\n",
        "      weights2 = param.data\n",
        "\n",
        "  weights1 = weights1.squeeze().cpu() #sorting runs faster on CPU\n",
        "  weights2 = weights2.squeeze().cpu()\n",
        "\n",
        "  for k in range(0, weights1.shape[0], 1):\n",
        "    for j in range(0, weights1.shape[1], 1):\n",
        "      print(\"conv1.weight[\"+str(k) + \"][\"+str(j)+\"] similar words: \")\n",
        "      #weightsCopy = weights1.detach().clone()\n",
        "      dists = torch.cosine_similarity(glove.vectors.cpu(), weights1[k][j].cpu())    # compute distances to all words\n",
        "      lst = sorted(enumerate(dists), key=lambda x: x[1]) # sort by distance\n",
        "      for i in range(len(lst) -2, len(lst)- (5+2), -1):    # take the top n, don't consider top result (will be target itself)\n",
        "        idx = lst[i][0]\n",
        "        difference = lst[i][1]\n",
        "        print(glove.itos[idx], \"\\t%5.2f\" % lst[i][1])\n",
        "\n",
        "\n",
        "  for k in range(9, weights2.shape[0], 1):\n",
        "    for j in range(0, weights2.shape[1], 1):\n",
        "      print(\"conv2.weight[\"+str(k) + \"][\"+str(j)+\"] similar words: \")\n",
        "      #weightsCopy = weights2.detach().clone()\n",
        "      dists = torch.cosine_similarity(glove.vectors.cpu(), weights2[k][j].cpu())    # compute distances to all words\n",
        "      lst = sorted(enumerate(dists), key=lambda x: x[1]) # sort by distance\n",
        "      for i in range(len(lst) -2, len(lst)- (5+2), -1):    # take the top n, don't consider top result (will be target itself)\n",
        "        idx = lst[i][0]\n",
        "        difference = lst[i][1]\n",
        "        print(glove.itos[idx], \"\\t%5.2f\" % lst[i][1])\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "8uzJVpb5IWLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_closest_words(net)\n",
        "net = train(k1 = 2, k2 = 16, n1 = 15, n2 = 15, epochs = 50, lr = 0.0001, train_dataloader = train_dataloader, val_dataloader = val_dataloader, test_dataloader = test_dataloader)\n",
        "torch.save(net.state_dict(), 'CNN_classifier_model.pt')"
      ],
      "metadata": {
        "id": "FBtX2_6Wh7FK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}