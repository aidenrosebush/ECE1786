# -*- coding: utf-8 -*-
"""A1P4_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z-W4Ca5CTrLvtdAKFXJfGSG-9aqEnXTD
"""

def train_sgns(batchsize, epochs, learning_rate, textlist, window, embedding_size):
  # Set up a model with Skip-gram with negative sampling (predict context with word)
  # textlist: a list of strings

  # Create Training Data
  X, T, Y = tokenize_and_preprocess_text(textlist, w2i, window)

  randomIndices = torch.randperm(X.shape[0]) #randomize for good measure

  X = X[randomIndices]
  Y = Y[randomIndices]

  # Split the training data
  train_samples = X[:int(0.8*len(X))]
  val_samples = X[int(0.8*len(X)):]

  train_labels = Y[:int(0.8*len(X))]
  val_labels = Y[int(0.8*len(X)):]

  # instantiate the network & set up the optimizer

  network = SkipGramNegativeSampling(len(w2i), embedding_size)
  optimizer = torch.optim.Adam([network.embedding], lr = learning_rate)
  sig = torch.nn.Sigmoid()

  tlosses = []
  vlosses = []
  epochsList = np.linspace(0,epochs-1, epochs)

  for i in range(epochs):
    print("Epoch "+str(i))
    tloss = 0.0
    vloss = 0.0
    word_index = 0 #writing this differently than in part 3- just keeping track of which word...
    #... i'm looking at in the corpus
    while word_index < len(train_labels):
      x = train_samples[word_index:min(word_index+batchsize, len(train_samples))].reshape(2,min(batchsize, len(train_samples)-(word_index)))[0]
      t = train_samples[word_index:min(word_index+batchsize, len(train_samples))].reshape(2,min(batchsize, len(train_samples)-(word_index)))[1]
      y = train_labels[word_index:min(word_index+batchsize, len(train_labels))] #labels
      optimizer.zero_grad()
      p = network(x, t)
      l = torch.mean(torch.log(sig(y*p) + 1e-5)) #average the losses (the purpose of batching)
      l.backward()
      optimizer.step()
      word_index += batchsize
      tloss += l.item()/len(train_labels)
    vshape = val_samples.shape
    xv = val_samples.reshape(vshape[1], vshape[0])[0]
    tv = val_samples.reshape(vshape[1], vshape[0])[1]
    yv = val_labels
    pv = network.forward(xv,tv)
    lv = torch.mean(torch.log(sig(yv*pv) + 1e-5))#average the losses
    vloss = lv.item()/len(val_labels)

    tlosses += [tloss]
    vlosses += [vloss]

  fig, ax = plt.subplots()
  ax.plot(epochsList, tlosses, label = 'Training Loss')
  ax.plot(epochsList, vlosses, label = 'Validation Loss')
  ax.set_xlabel('Epoch Number')
  ax.set_ylabel('Log-sigmoid loss')
  ax.legend()

  return network